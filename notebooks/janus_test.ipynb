{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de16b82b-20f6-4746-a991-c82db80b809b",
   "metadata": {},
   "source": [
    "### Load model and processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "195c6ef3-45b5-4133-8ba3-40dcf05914f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# For interactive environments like Jupyter Notebook\n",
    "current_dir = os.getcwd()  # Use the current working directory\n",
    "janus_folder = os.path.join(current_dir, \"Janus\")\n",
    "if janus_folder not in sys.path:\n",
    "    sys.path.insert(0, janus_folder)\n",
    "\n",
    "def load_janus_pro(\n",
    "    model_id: str = \"deepseek-ai/Janus-Pro-7B\",\n",
    "    return_attention: bool = False,\n",
    "    return_logits: bool = False,\n",
    "    quantization: str = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Load the Janus Pro multi-modality model and its associated VLChatProcessor.\n",
    "\n",
    "    Args:\n",
    "        model_id (str): Identifier or path of the Janus Pro model.\n",
    "            For example: \"deepseek-ai/Janus-Pro-1B\" or \"deepseek-ai/Janus-Pro-7B\".\n",
    "        return_attention (bool): If True, the processor/model will be configured to return attention maps.\n",
    "        return_logits (bool): If True, the model will be configured to return logits/scores.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - model: The loaded MultiModalityCausalLM model.\n",
    "            - processor: The loaded VLChatProcessor.\n",
    "            - tokenizer: The tokenizer used by the VLChatProcessor.\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    from transformers import AutoModelForCausalLM\n",
    "    # Import the Janus objects from your package\n",
    "    from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "\n",
    "    # Load the processor (this includes loading the tokenizer)\n",
    "    processor = VLChatProcessor.from_pretrained(\n",
    "        model_id\n",
    "    )\n",
    "    \n",
    "    tokenizer = processor.tokenizer\n",
    "\n",
    "\n",
    "    # Configure quantization\n",
    "    bnb_config = None\n",
    "    if quantization == \"4b\":\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "    elif quantization == \"8b\":\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            bnb_8bit_use_double_quant=True,\n",
    "            bnb_8bit_quant_type=\"nf4\",\n",
    "            bnb_8bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "    elif quantization == \"16b\":\n",
    "        torch_dtype = torch.bfloat16\n",
    "    else:\n",
    "        torch_dtype = torch.float32\n",
    "\n",
    "\n",
    "    # Load the model using Hugging Face's AutoModelForCausalLM.\n",
    "    # Here, trust_remote_code=True is used because Janus may contain custom model code.\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        trust_remote_code=True,\n",
    " \n",
    "        device_map=\"cuda\",\n",
    "        quantization_config=bnb_config if bnb_config else None,\n",
    "        torch_dtype=torch.bfloat16 if quantization in [\"4b\", \"8b\"] else torch_dtype,\n",
    "        output_attentions=return_attention,\n",
    "        output_scores=return_logits,\n",
    "        return_dict_in_generate=True\n",
    "    )\n",
    "\n",
    "    # Move model to GPU using bfloat16 and set it to evaluation mode.\n",
    "    model = model.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "    return model, processor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de571ec9-3010-4ec8-9388-2de616a6f5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/users/restrepoda/.conda/envs/base_ml/lib/python3.12/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version is above 3.10, patching the collections module.\n"
     ]
    }
   ],
   "source": [
    "model, processor = load_janus_pro(model_id=\"deepseek-ai/Janus-Pro-1B\",quantization=\"16b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a499d0be-8f9f-4641-806d-cc08caa76956",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b47f37b-3b83-424f-8a58-dd38824b778c",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1610aa4-bbca-4c05-baea-734df4183257",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from janus.utils.io import load_pil_images\n",
    "from PIL import Image\n",
    "\n",
    "question = \"Given the image. Describe this image\"\n",
    "\n",
    "image = 'Janus/images/equation.png'\n",
    "image = Image.open(image)\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"<|User|>\",\n",
    "        \"content\": f\"<image_placeholder>\\n{question}\",\n",
    "        \"images\": [image],\n",
    "    },\n",
    "    {\"role\": \"<|Assistant|>\", \"content\": \"\"},\n",
    "]\n",
    "\n",
    "# load images and prepare for inputs\n",
    "#pil_images = load_pil_images(conversation)\n",
    "\n",
    "prepare_inputs = processor(\n",
    "    conversations=conversation, images=[image], force_batchify=True\n",
    ").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac8a713-41f9-44f0-9cb7-08aea7f8abed",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2216b75e-dab7-48f4-b35b-fdc8f86e2c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence shape:\n",
    "prepare_inputs.images_seq_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1d8731-124c-42f3-b5a3-1e23a6316847",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d6c5b7-e2a7-43ef-aec7-6e209d049c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate image and text embeddings\n",
    "inputs_embeds = model.prepare_inputs_embeds(**prepare_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4a6d6b-b737-418c-af42-f9073d893122",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.prepare_inputs_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3d9146-b5c2-4062-aff7-8f63e2132e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence embedding shape\n",
    "inputs_embeds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56467ba1-e424-465e-b4c9-f03964a8e3d8",
   "metadata": {},
   "source": [
    "### Text generation LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faf73db-2c81-4a96-a86b-fefe58f3ed28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.language_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa308f6f-3b9f-4282-892e-e36a1e094eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.language_model.generate(\n",
    "    inputs_embeds=inputs_embeds,\n",
    "    attention_mask=prepare_inputs.attention_mask,\n",
    "    pad_token_id=processor.tokenizer.eos_token_id,\n",
    "    bos_token_id=processor.tokenizer.bos_token_id,\n",
    "    eos_token_id=processor.tokenizer.eos_token_id,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    "    use_cache=True,\n",
    "    output_attentions=True,\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88fb279-3fb4-4b42-ade8-524bf0b79a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92deb865-a14c-4240-bdd8-0b5eae3f395a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.scores[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd578b13-51d5-4435-ba09-f3197c46640f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(outputs.scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5afc59a-6f10-4c70-8920-02c5927713aa",
   "metadata": {},
   "source": [
    "### Decode output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff186cf-fb45-4efa-b5c1-91cb59b52a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b0eccc-df8c-4dd9-9f0e-f0e8a0ea87cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.tokenizer.decode(outputs.sequences[0].cpu().tolist(), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770df0db-2e2a-41e4-b89a-1229c122475a",
   "metadata": {},
   "source": [
    "### Remove and empty cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce9e984-473c-45f8-977f-25fc1fbfff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "del model, processor\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288e968a-4e94-4064-bba9-981a0019fef8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
